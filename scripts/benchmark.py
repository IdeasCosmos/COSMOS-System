"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import time
import statistics
import aiohttp
import numpy as np
from typing import List, Dict

class BenchmarkRunner:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results = []
    
    async def run_single_request(self, session: aiohttp.ClientSession, text: str) -> Dict:
        """ë‹¨ì¼ ìš”ì²­ ì‹¤í–‰"""
        start = time.perf_counter()
        
        payload = {
            "text": text,
            "enable_resonance": True
        }
        
        headers = {"X-API-Key": "dev-key"}
        
        async with session.post(
            f"{self.base_url}/analyze-resonance",
            json=payload,
            headers=headers
        ) as response:
            result = await response.json()
            latency = (time.perf_counter() - start) * 1000
            
            return {
                "latency_ms": latency,
                "status": response.status,
                "result": result
            }
    
    async def run_concurrent_requests(
        self,
        texts: List[str],
        concurrency: int = 10
    ) -> List[Dict]:
        """ë™ì‹œ ìš”ì²­ ì‹¤í–‰"""
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            for i in range(0, len(texts), concurrency):
                batch = texts[i:i+concurrency]
                batch_tasks = [
                    self.run_single_request(session, text)
                    for text in batch
                ]
                results = await asyncio.gather(*batch_tasks)
                tasks.extend(results)
                
                # ì¿¨ë‹¤ìš´
                await asyncio.sleep(0.1)
            
            return tasks
    
    async def benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ Starting COSMOS Benchmark")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_texts = [
            "ì˜¤ëŠ˜ ì •ë§ ê¸°ì˜ë„¤ìš”!",
            "ìŠ¬í”ˆ ì†Œì‹ì„ ë“¤ì—ˆì–´ìš”...",
            "í™”ê°€ ë‚˜ì„œ ì°¸ì„ ìˆ˜ê°€ ì—†ì–´",
            "ë¬´ì„œìš´ ì˜í™”ë¥¼ ë´¤ëŠ”ë° ì •ë§ ë¬´ì„œì› ì–´ìš”",
            "ë†€ë¼ìš´ ë°œê²¬ì„ í–ˆìŠµë‹ˆë‹¤",
            "í‰ì˜¨í•œ í•˜ë£¨ë¥¼ ë³´ë‚´ê³  ìˆì–´ìš”",
            "ë³µì¡í•œ ê°ì •ì´ êµì°¨í•˜ëŠ” ìˆœê°„ì´ì—ìš”",
            "í•œêµ­ íŠ¹ìœ ì˜ ì •ì´ ëŠê»´ì§€ë„¤ìš”",
            "ì°¸ ì˜í–ˆë„¤ìš”... (ë°˜ì–´ë²•)",
            "ë©˜ë¶•ì´ ì™”ì–´ìš” í˜„íƒ€ê°€ ì™€ìš”"
        ] * 10  # 100ê°œ ìš”ì²­
        
        # ì›Œë°ì—…
        print("ğŸ”¥ Warming up...")
        await self.run_concurrent_requests(test_texts[:5], 1)
        
        # ë¶€í•˜ í…ŒìŠ¤íŠ¸
        for concurrency in [1, 5, 10, 20]:
            print(f"\nğŸ“Š Testing with concurrency: {concurrency}")
            
            start = time.time()
            results = await self.run_concurrent_requests(
                test_texts,
                concurrency
            )
            total_time = time.time() - start
            
            # í†µê³„ ê³„ì‚°
            latencies = [r["latency_ms"] for r in results]
            successful = sum(1 for r in results if r["status"] == 200)
            
            stats = {
                "concurrency": concurrency,
                "total_requests": len(results),
                "successful": successful,
                "total_time": total_time,
                "rps": len(results) / total_time,
                "latency_mean": statistics.mean(latencies),
                "latency_median": statistics.median(latencies),
                "latency_p95": np.percentile(latencies, 95),
                "latency_p99": np.percentile(latencies, 99)
            }
            
            self.print_stats(stats)
            self.results.append(stats)
        
        return self.results
    
    def print_stats(self, stats: Dict):
        """í†µê³„ ì¶œë ¥"""
        print(f"  âœ… Success: {stats['successful']}/{stats['total_requests']}")
        print(f"  âš¡ RPS: {stats['rps']:.2f}")
        print(f"  â±ï¸ Latency (ms):")
        print(f"    - Mean: {stats['latency_mean']:.2f}")
        print(f"    - Median: {stats['latency_median']:.2f}")
        print(f"    - P95: {stats['latency_p95']:.2f}")
        print(f"    - P99: {stats['latency_p99']:.2f}")

async def main():
    runner = BenchmarkRunner()
    results = await runner.benchmark()
    
    print("\n" + "="*50)
    print("ğŸ“ˆ Benchmark Complete!")
    print("="*50)
    
    # ëª©í‘œ í™•ì¸
    targets = {
        "latency_p50": 50,  # ëª©í‘œ: <50ms
        "latency_p99": 100,  # ëª©í‘œ: <100ms
        "rps": 100  # ëª©í‘œ: >100 RPS
    }
    
    best_result = max(results, key=lambda x: x['rps'])
    
    print(f"\nğŸ† Best Performance:")
    print(f"  - RPS: {best_result['rps']:.2f} (Target: >{targets['rps']})")
    print(f"  - P50 Latency: {best_result['latency_median']:.2f}ms (Target: <{targets['latency_p50']}ms)")
    print(f"  - P99 Latency: {best_result['latency_p99']:.2f}ms (Target: <{targets['latency_p99']}ms)")
    
    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
    if (best_result['latency_median'] < targets['latency_p50'] and
        best_result['latency_p99'] < targets['latency_p99'] and
        best_result['rps'] > targets['rps']):
        print("\nâœ… All performance targets met!")
    else:
        print("\nâš ï¸ Some targets not met. Consider optimization.")

if __name__ == "__main__":
    asyncio.run(main())